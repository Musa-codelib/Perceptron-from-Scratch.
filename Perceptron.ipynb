import numpy as np

class Perceptron:
    def __init__(self, num_features, learning_rate=0.1, threshold=0.0, max_epochs=100):
        self.num_features = num_features
        self.learning_rate = learning_rate
        self.threshold = threshold
        self.max_epochs = max_epochs
        self.weights = np.random.rand(num_features)
    
    def step_function(self, z):
        return 1 if z >= self.threshold else 0
    
    def train(self, X, y):
        for epoch in range(self.max_epochs):
            errors = 0
            for i in range(len(X)):
                # Compute the weighted sum
                z = np.dot(X[i], self.weights)
                
                # Apply the step function
                y_pred = self.step_function(z)
                
                # Calculate the prediction error
                error = y[i] - y_pred
                
                # Update weights
                self.weights += self.learning_rate * error * X[i]
                
                errors += error

            # If there are no errors, stop training
            if errors == 0:
                print(f"Training converged after {epoch + 1} epochs.")
                break

    def predict(self, X):
        predictions = []
        for i in range(len(X)):
            z = np.dot(X[i], self.weights)
            y_pred = self.step_function(z)
            predictions.append(y_pred)
        return predictions

# Example usage:
if __name__ == "__main__":
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features
    y = np.array([0, 0, 0, 1])  # Binary labels

    perceptron = Perceptron(num_features=2)
    perceptron.train(X, y)

    test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    predictions = perceptron.predict(test_data)

    for i, prediction in enumerate(predictions):
        print(f"Input: {test_data[i]}, Predicted Output: {prediction}")
